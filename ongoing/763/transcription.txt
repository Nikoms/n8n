It's always interesting to learn how other organizations approach software development, particularly when they're good at it, and even better when they have data and evidence to show how what they're doing really works. And even more fun from my point of view if they're reinforcing the practices that we recommend here on the modern software engineering channel every week. This week then, we'll be exploring how one well-known company made changes that reduce their average lead time of a feature to less than onethird of what it was before while at the same time reducing the number of bugs in production and increasing the number of features per release. So quite literally creating better software faster, the tagline for our channel. [Music] Hi and welcome to the modern software engineering channel. I'm Dave Farley and if you haven't been here before, please do hit subscribe and if you enjoy the content today, hit like as well. And if you really enjoy the content, why not tell us in the chat below this video because that helps the YouTube algorithm to recommend our videos to other people. I was provided with the data that I'm going to show you here today by a viewer of this channel who works as an engineering lead in mobile app development for this company and he's one of the people that helped them to try these new things to improve their practices. The company in question has kindly given me their permission to share this information with you, but only if I don't identify them. So, of course, I'm going to respect their instructions. Although I confess that in their place, I think I'd be rather proud to announce to the world that I was now doing a better job than many similar organizations and I think it's a very credable thing on their part that they are willing to help others to learn how to improve by sharing their experience. So once again, I thank them for sharing that experience. This company is a modern, very well-known, very successful web-based company. Initially, their development process was described to me as working as a kind of mini waterfall managing releases via Jira and GitHub that many of us are probably familiar with using a deployment pipeline initially based on CircleCI and later implemented as GitHub actions. They released new versions of their Android and iOS applications every couple of weeks. The mini waterfall was centered around these releases. each one comprising of a few new features with the developer working on a new feature on average for 151 hours. When the feature was thought to be complete by the developer, they create a pull request on average consisting of around 36 changed files. Two developers would review the pull request and after both had reviewed and approved these changes, they were then added to a queue that would wait to be picked up by a QA engineer. The QA engineer would pick up the pull request and check it with a combination of automated and manual tests. I don't have any detail on what this testing looked like in this case, but generalizing based on what I've seen, what most organizations that work this way do is to select both manual and automated tests from a list of some kind based on the nature of the change. There's nothing inherently dumb about that or this approach except that I think that dependence on manual testing for regression tests is bad and low quality as an idea. But this approach to test avoidance makes perfect sense if testing is slow and expensive. Clearly though, this approach introduces the risk that we may miss something and accidentally skip a test that would otherwise have found a problem had it been run. Let's pause there and say thanks to our sponsors. We're extremely fortunate to be sponsored by Equal Experts, Transfix, and Mail Trap. Mail trap is an email platform that developers love for fast email delivery, high inboxing rates, and live 24 by7 expert supports. Click on the details in the description below to check out those services. All of the companies offer products and services though that are very well aligned with the topics that we discuss here on the channel every week. So, if you're looking for excellence in continuous delivery and software engineering, do click on the links for any of our sponsors. In reality, we are always exposed to this risk of missing something if we don't always run every test after every change. So, we're left with a choice to either optimize to make our testing, whatever form it takes, fast, efficient, and lowcost enough so that we can afford to run every test after every small change, however small, or rely on the thoroughess of our analysis of which tests to avoid running. but always being at the risk that we may miss running the very test that would have spotted the mistake in this particular change. I confess that test avoidance strategies always leave me feeling a little bit nervous. In general, it feels more straightforward, easier, and safer to me to run all of the tests all of the time and optimize them to run quickly enough for that to work. I think it's important to remember though that whatever our approach, testing is never perfect. Much better to treat it as a falsification mechanism. Proof that our change is wrong rather than attempting to use the test to prove that our change is good. After all, however many tests we have, we may always be missing an important one, either by avoiding running it or by forgetting to have written it in the first place. Back to our case study, though. Once these selected tests are passing, the change is then promoted to a new queue. This time we're waiting for a design review. After being checked by the designers, the change is then merged to a development branch and would then have to wait to be added to another branch ready for release. So there's quite a lot of cues and branches and quite a lot of waiting involved here. Lots of places then to optimize this process. Overall, the average life cycle of a pull request in this process was 253 hours. Certainly a long way from what we would think of as any form of continuous delivery, where we generally aim to establish the releaseability of our system, at least daily. My friend describing this to me was familiar with and inspired by Dragon Steepanovich's work. I spoke to Dragon about his analysis of over 40,000 pull requests and what he learned from them in this video. To summarize Dragon's findings, he saw that if you make a 10line change to some code, you likely to get 10 review comments. But if you made a 100line change to code, you'd probably get one comment that said something like, "All looks good to me." The bigger and more complex the change, the less serious the review and the less value it added. Or to put it another way, code review on the basis of too long didn't read. If this approach to reviewing code had really worked, then what we'd expect to see is a correlation of some kind between the number of files changed and the average number of comments. So, a roughly linear trend. But that's not a dragon found at all. My friend at our target company here decided to try to perform a similar analysis on the pull requests in his company and he found almost exactly the same thing. He examined a few different repositories, each one containing more than 5,000 files. These repositories seemed to mirror Dragon's findings. The more complex the change, the less useful, the less protection offered by the review comments. At first look, two repositories seemed to buck the trend a little. There was a hint of a more linear relationship going on between the number of files changed and the number of review comments, but on closer examination, this was an illusion. It turned out that these teams were using a tool called Code Rabbit that automatically commented on their changes. Our researcher thought that maybe these teams had found a useful tool that was helping them to highlight real problems. But in practice, the vast majority of these automated comments were marked by the developers as span. So served no real useful purpose and so only added a bit more work into the process. So Dragon's findings were repeated in this company too. The more code was changed, the fewer the comments. So at the instigation of our researcher, the teams agreed to try something different. They looked into pair programming and faced the usual nervousness from managers and probably from other developers too. The manager assumed what seems obvious when you talk about pair programming that if two people work together, they're going to finish half as many features. But also that if there's no pull requests, code quality is going to go down. And as a consequence of that, he expected to see bug counts increase. But to his credit, he was open to trying the experiment. He didn't prejudge it. So teams agreed that a minimum of 30% of their features would be created via pair programming. Any feature that was developed entirely via pair programming could skip any other code review process since it was already being reviewed by the pair. And everyone agreed that they'd monitor the impact of this every week so that if the number of bugs went up or the quality started to go down, they'd stop the experiment. These fears were in practice completely unfounded but also normal in my experience. Pair programming is an underused but extremely valuable approach. I talk with Ascot and Ola from Sparebank 1 in Norway about them dropping poll requests and adopting pair programming instead in this video. In the end, the use of pair programming in today's target company went much better than they had expected. So they ended up continuing their experiment for the next 100 days before agreeing to continue with it after that. Their results were very good indeed. I confess to being very pleased that this company and our intrepid researcher succeeded, but I'm also, to be honest, not at all surprised. Pair programming is extremely effective as a practice, but is all too often overlooked by nearly everyone. This is a shame because the evidence and the data for it are really rather strong in its favor. I talk about the pros and cons of pair programming in this video, a lot of videos today, and about the disconnect between what people think before they've tried it and then what they think afterwards. After 100 days, the average development time per feature went from 151 hours down to 44. A pretty dramatic improvement. This more than diffusing any concerns over two people doing the work of one. The median time per feature went from 44 to 22 hours. So even ignoring outliers, raw productivity levels were at least maintained feature by feature. But as I have said before in some of these other videos, raw productivity is really the wrong metric. We need to take a broader view. While our friendly researcher did find that defects rates reduced as a result of pair programming, he didn't say by how much and he didn't say anything about other measures of quality in the code after their experiment either. Data from other studies though is generally very good on this. All of the studies that I've ever seen report that the quality of the output from pairs is measurably often significantly better than output of developers working alone. These measurements are made in a variety of ways depending on the study tests passing, simplicity of the code, readability functionality correctness speed, and the confidence of the developers that they have in the code to name but a few. Disconnecting pair programming from other practices though is often rather difficult. Teams that work the way that I recommend and score highly against the doraometrics of stability and throughput spend 44% more time on new features than teams that don't. But this is for a wide variety of deeply interlin reasons, but not least because teams like this simply produce dramatically fewer bugs. Teams often see more than a 90% reduction in defects in production with these practices. and when they are all combined together and so spend much less time on tracking, documenting and fixing bugs in production and instead spend that time working on new features. In addition to these efficiency gains, the team here were also able to largely eliminate the overheads and costs of all of those cues and weights in the process. So the average pull request cycle cost went down from 253 hours to 130. There is more scope for improvement here though more work to do and the team is looking now into experimenting with adding trunkbased development, test-driven development and acceptance testdriven development. All practices that I would strongly recommend which I would expect should largely help them to do away with those pull request life cycle costs altogether. Overall, during their 100day experiment, the team found that all of their fears were unfounded. Not only was the team now more productive with fewer unproductive delays and cues in the process, but there were also fewer bugs reported and quality increased. This is completely in line with my experience of teams that adopt pair programming. I'd also bet that over time their team culture will strengthen and improve. they will be more willing to try out these new ideas. And I'd expect to see quality and productivity increase even more than it has already. I'd like to thank our mystery company and our mystery researcher for allowing us to share their learning. I'd also like to encourage you to try pair programming. For the vast majority of people, it's a lot more fun than you think. And if you're a manager or an employer of developers, I'd like to reinforce the message here. Counterintuitive as it may seem, pair programming is a more productive, higher quality way of working and one that's repeatably backed up by academic research and by practical experience. I've added a few links in the description to this video below. So, do check them out. Thank you very much for watching our stuff here on the modern software engineering channel. And if you enjoy our stuff, why not support us by joining our Patreon community? And to our existing Patreon members, as ever, thank you very much indeed for your support. Thank you and bye-bye. [Music]