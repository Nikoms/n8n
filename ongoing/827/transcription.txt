En 2025, j'ai livré plus de 15 projets rag. Certains ont généré plus de six chiffres pour mes clients. D'autres m'ont appris des leçons brutales. Après 8 ans de projet tech, data et un taux de déploiement de 93 %, je vais te donner ce que personne partage, les vraies leçons terrain, pas les tutoriels YouTube. Ce que tu vas voir dans cette vidéo va te faire économiser 6 mois de galère. Déjà, le réflexe classique quand un rag fonctionne mal, c'est de suite de se demander si le problème vient pas du modèle, si le problème vient pas de la façon dont on a chunké nos informations de notre modèle d'embing et cetera. Mais la réalité c'est que 80 % des problèmes viennent de la data ou de l'interface. Côté data, on va souvent par exemple avoir des PDF qui sont juste extraits en texte dans lequel on a aussi de l'information qui peut être sous forme d'image, de schémas, de photos de texte et cetera. Et en fait, on finit avec des PDF qui sont mal extraits et dans lequel il nous manque de l'information. Donc un PDF en fait extrait sans OCR, ben souvent on va avoir un rag qui recherche dans le vide quoi. Donc dès qu'on n' pas que du texte pur, l'idée est quand même de passer sur un bon modèle haussière. Deuxième chose sur la data, c'est la gestion des doublons. Là c'est pareil on va souvent avoir dans un corpus de données pas mal d'informations qui se répètent. Là, on va chercher l'origine de l'information maîtresse et quelle est l'information qui prévaut sur toutes les autres. Autre chose qui est très liée c'est la gestion du bruit. souvent, on va mettre tous nos PDF, toute notre data brutement dans notre système. On va pas enlever le bruit et en fait, on va se retrouver avec euh 30 % de données utiles et 70 % de bruit et après, on s'étonne que nos rag ne fonctionnent mal. Donc ça c'est super important c'est de nettoyer la donnée quoi. Avant dernier élément absence de métadonnée. Si on nenrichit pas avec de la métadonnée, dès qu'on va passer avec du volume, on va vraiment complexifier le travail des bases de données vectorielles et du LLM pour définir quelle est l'information pertinente. Et enfin, de la donnée incomplète, ça arrive très souvent. Si la réponse à une question n'est pas présente dans la donnée accessible, ben il est évident qu'on aura du mal à apporter une réponse. Côté interface, donc le rag, il fonctionne parfaitement mais l'utilisateur, il sait pas quoi demander. Il se retrouve devant le chatbot, il sait pas comment formuler la question, il ne sait pas quel niveau de contexte il doit donner et donc bien évidemment on a des règles qui nous donnent pas la réponse attendue. On se retrouve avec un champ de texte vide et là l'utilisateur bloque. Si l'interface ne sert pas à guider l'utilisateur et à récupérer le contexte manquant, la solution aura très peu de chance d'être adoptée. Et enfin, je dirais le syndrome du chatbot magique. On va souvent avoir des utilisateurs qui s'attendent à ce que Lia devine leurs besoins. Et donc la vérité, c'est qu'un rag moyen avec une bonne interface va toujours battre un rag parfait mais une interface nulle. Et donc les leçons sur ces éléments-là c'est que avant de toucher au retrieval donc la récupération d'information audit ta data, ça pourra sauver ton projet. Avant de livrer, teste avec des vrais utilisateurs, pas juste les métriques techniques. On a parlé de Golden Dataset dans les autres vidéos mais rien ne vaut les tests avec les vrais utilisateurs. La question super importante ici, c'est est-ce que l'utilisateur, il sait formuler la question pour avoir les bonnes réponses ou est-ce qu'il a besoin d'être guidé pour donner un peu plus d'informations ? Alors, une fois que la data est propre et que l'interface guide l'utilisateur il y a un outil qui change tout. Et cet outil, c'est le Golden Dataset. Le Golden Dataset, c'est une suite de 15 à 50 questions réponses. C'est-à-dire qu'on a un échantillon de quelles sont les questions possibles de l'utilisateur et les réponses attendues qui va nous permettre de mesurer la performance de notre rag à plusieurs étapes qui va nous permettre de définir le data model. Donc, c'est-à-dire en fonction des réponses attendues et de la data disponible, quel est le modèle de données euh qu'on doit créer donc avec la métadonnée qui va bien et cetera qui va nous permettre à partir de ces questions utilisateurs de tomber sur les bonnes réponses à l'échelle puisqu'on parle d'un échantillon et on doit extrapoler euh tout un tas de questions et de réponses par la suite. Ça va également nous permettre de réaliser des évaluations dans les tests en charge avant livraison pour savoir si notre système est performant. Alors, comment on construit un Golden dataset avec son client ? On va prendre trois niveaux de questions-réponses et questions faciles questions moyennement difficiles et des questions difficiles ou complexes à répondre. Et pour ça, on va définir quelle est la structure de la réponse comment elle doit être affichée cette réponse et quels sont les différents niveaux d'information en fonction de la difficulté. Vous avez besoin de quelqu'un qui connaît suffisamment bien la problématique que vous adressez pour être capable de savoir quelle est une bonne réponse et si on a tous les éléments nécessaires dans la donnée disponible. Alors, l'erreur fatale ici c'est d'inventer les questions soi-même. Là, on va tomber dans des billets de confirmation. On va regarder la data, on va imaginer des questions mais sans connaître le besoin utilisateur. C'est le meilleur moyen de se planter. Comme on dit, vaut mieux un qui sait que deux qui cherchent. Et une fois que tu as ça ce qui est bien, c'est que tout le reste devient mesurable. Alors le chapitre 3 c'est la configuration de base qui marche dans 70 % des cas PME. Une fois que ta data, elle est propre et que tu as un Golden data, je dirais que l'étape numéro 1 avant de complexifier ton système et d'avoir des stratégies un peu trop avancées, structurer sa donné sémantiquement pour que à peu près chaque chunk puisse être coupé naïvement. Utiliser un embedding hybride. Donc là, c'est sémantique plus mot clé. Si ces concepts-là sont déjà un peu trop avancés, n'hésitez pas à regarder mes vidéos précédentes. On aborde tous ces sujets en détail. Et enfin, rajouter un reanurer pour venir en fait mettre au-dessus de la pile les chunks les plus pertinents. Donc entre un chunking naïf, un embedding hybride et le reanurer, on a une baseline qui fonctionne bien dans vraiment beaucoup de cas de PME. Dans tous les cas, même si vous devez en venir à des stratégies plus avancées, c'est une bonne façon de démarrer afin de venir mesurer et savoir à quel endroit vous avez besoin d'augmenter les performances. Alors même quand tu as une bonne config, il y a quand même une étape que tout le monde bacle et cette étape l'évaluation. Le problème ici, c'est que tout le monde teste un peu à l'œil. Ça semble marcher mais on sait pas exactement le mesurer précisément. Et pour ça, il y a trois critères super importants. Le premier critère, c'est la précision. Ça semble assez évident. Est-ce que le LLM me donne une réponse précise, complète et répétable ? Ensuite, on a le métrique du recall. Est-ce que mon système trouve les bons chunks dans ma base de connaissance ? Là, ce qu'on veut savoir ici, c'est est-ce que les bonnes informations sont bien récupérées à partir de la question utilisateur ? Et enfin, il y a une autre métrique qu'on appelle faithfulness. Est-ce que la réponse, elle est fidèle aux sources ? Est-ce qu'on est capable de sourcer l'information, page, document et cetera ? Est-ce qu'on est capable de la vérifier ? Est-ce qu'elle est bien fidèle à l'information dans notre base vectorielle ou est-ce qu'elle a été sujet à une hallucination ? C'est-à-dire que grâce aux données d'entraînement elle a donné la bonne réponse. Mais la bonne réponse ne vient pas de la base de connaissance. Ça, c'est un cas qui peut arriver parfois. Pourquoi c'est dangereux si on fait pas ça ? On peut avoir un système qui donne la bonne réponse 10 fois et c'est la 11e fois qui plante. Nous ce qu'on veut c'est pouvoir évaluer un minimum d'échelle pour être sûr de la performance de notre système. Mon process ici c'est que je passe dans le Golden dataset pour venir scorer mes réponses et avoir un résultat en pourcentage. Et donc maintenant les erreurs que j'ai vu en 2025. Alors première erreur, c'est ce qu'on pourrait appeler le syndrome YouTube. On fait un test sur 10 documents, ça marche, on se dit qu'on peut le mettre en prod et là forcément ça marche plus. Numéro 2 c'est que les entreprises et les providers pensent souvent mesurer la valeur alors qu'en fait ils ne mesurent que la faisabilité technique. Ensuite erreur numéro 3, le syndrome, on a toute la data. Donc, on a un client ou des métiers qui affirment avoir toute la data mais personne n allé réellement fouiller dans les dossiers pour savoir exactement, ce, qu'il, y avait., Résultat, on se retrouve à mi projet avec 60 % de la donnée qui est inutilisable. Là, le projet est un peu foutu. Donc le son ici, c'est qu'on commence toujours par un audit de la data. Il vaut mieux prendre un ou deux jours en amont que perdre 3 semaines ou 3 mois car la donnée est inutilisable. Erreur numéro 4, tout vouloir faire dès le départ. Donc, on veut un rag sur tous nos documents. On a 5000 documents, 15 formats différents, zéro structure. Résultat, on a un scope ingérable et le projet sans lise. La leçon ici, c'est commencer par un cas d'usage, un périmètre, prouver la valeur et enfin élargir. Problème numéro 5, sous-estimer l'importance de l'interface utilisateur. Le problème, c'est que si vous avez une interface chatbot, souvent vous allez avoir des utilisateurs qui savent pas quoi demander, qui se retrouvent bloqués et qui l'adoptent pas. Et même si votre RAC fonctionne super bien mais qu'il est utilisé que par 20 % des utilisateurs cible, ben votre projet il sera jugé comme un échec. La leçon ici, c'est que votre interface doit guider l'utilisateur pour donner ce contexte et donner ses informations afin qu'il puisse obtenir les bonnes réponses. Erreur maintenant côté fournisseur de solutions et ce que je vois vraiment très souvent, c'est l'over engineering c'est-à-dire vouloir construire des systèmes complexe en local avec beaucoup d'infrastructure avec des très gros périmètres et en fait on se retrouve à perdre énormément de temps sur tout ce qui gravite autour du rag sans jamais en fait venir itérer sur ce qui a vraiment de l'importance et ce qui a vraiment de la valeur. Et en soit on perd beaucoup de temps jusqu'à avoir beaucoup de projets qui vont pas au bout. Erreur numéro 2, ne pas proposer d'audit de la data. Voà, ça c'est un gros problème parce que si vous voulez avancer à l'aveugle, il y a pas mieux. Alors que comme je le disais précédemment, c'est quelque chose qui peut être assez rapide et vous donner de la clarté sur tout le reste du projet. Erreur numéro 3 qui est très liée, c'est promettre des délais très courts alors qu'on n pas vu la data. Voilà, c'est pareil, vous avez une chance sur deux d'aller au carton, quoi. Donc ces patterns, je les vois partout et voilà ce que 2025 m'a confirmé. Numéro 1 est très important. Les entreprises, elles veulent du résultat pas de la technologie. On veut pas d'époque preuve de concept pour montrer la faisabilité technique. On veut des preuves de valeur pour montrer que ça va être utilisé et qu'on a un potentiel ROI où on veut des preuves de business c'est-à-dire de créer des nouveaux services qui seront un à utiliser mais deux pour lesquels les utilisateurs sont prêts à payer. Ce qu'il y a presque de plus important, c'est surtout savoir quand et là je vais le mettre en gros ne pas faire de rag. Savoir quand ne pas faire de rag est presque plus important que de savoir quand en faire. L'idée importante, c'est de pas essayer de faire rentrer des rondes des carrés et de vouloir refourguer des solutions parce que on sait à peu près comment les faire. L'important, c'est surtout de savoir dans quel cas il y a pas d'autre choix que de faire du rag et donc là où ça aura réellement le plus de valeur. Maintenant, tu sais ce que ces deux dernières années m'ont appris sur le rag. La data, l'interface et les erreurs qui se répètent. Mais le rag, ce n'est qu'une brique. Si tu veux comprendre pourquoi 85 % des projets IA échouent pas juste les rages, tous les projets. Et comment éviter ces pièges avant même de commencer, j'ai fait une vidéo complète là-dessus. Le lien est juste ici.