[Music] Welcome and thank you everyone for taking the time um for coming to the talk. I know there are lots of other good sessions on I do really appreciate you coming to coming to this one. So this talk is going to be all about my experience over the what I used to say 10 years but it's getting closer to 20 as I get more gray hair. Um building applications as an application engineer and then more recently for the last 5 years or so building platforms that people deploy applications to. It'll take a lot of inspiration from resources like 12 factor apps. um if you if if you've read that and what I've spent a lot of my time doing or what always seems to happen is I'm helping people like migrate from older infrastructure it could be mainframe it could be VMs etc and this is the ever growing checklist of the things that I check for the applications before we put them on something like you know a container orchestrator or or or serverless it's really everything I want like all the developers that work in my team to know about application architecture for continuous delivery I'll start off just with the uh the briefest explanation of what I consider continuous delivery to be. So, you know, when we're talking about continuous delivery, it's really just about enabling change into production, but with speed and security and reliability built in. So, I I'll cover that in just a little bit of more detail before we go on to the application architecture. So, what do I mean by change? It's it's anything which makes up your production system. Today, we're really going to be focusing on applications, i.e. things that you own the code for and write the code for. But you know most of the time I'm also considering things like data pipelines batch processing infrastructure changes anytime we're configuring networks provisioning cloud infrastructure for me I see all these types of things as the same as well as vendor products. So if part of your system is made up of something you buy then I also consider the same kind of continuous delivery pipeline. I'm trying to design the same continuous delivery pipeline. And of course we're not just deploying things randomly to production. We want to do it at speed. That's typically the motivation for investing in continuous delivery efforts. Um, but we want to do these things securely. As the system gets bigger, as we get more engineers, as the system becomes more complex, we want to make sure we're not worsening the security of that system. And a lot of the talk today will be about how we how our application architecture affects the reliability. And then finally, most people care about auditability of continuous delivery. So that is what version goes into what environment when and and how did it get there. Uh I did a whole talk on like kind of covering all of those aspects yesterday. So feel free to go and watch that. Today uh the rest of the content is really just going to be about applications. So I'm assuming that you own the code and the life cycle of the thing that you're you're doing. It's not a bit of infrastructure. It's not a vendor product. And I'm going to introduce an architecture which I think covers 90% of the applications I've worked on in my life. That's not to say there's not a more niche 10% for very like high throughput or like trading systems and things like that, but I really want to focus on the the 90% style of application architecture. So I'm going to introduce a few terms which I think are pretty standard. But before we jump into what is an application, we'll just introduce the concept of a full system. So a full system in my view is something which serves a business need. It could be your product. It could be something like a TV streaming platform, a payment system, etc. And it's typically made up of many parts. You know, you've got your ingress and client applications. So these are the people actually using using the product. It then typically goes through some kind of load balancing, you know, global load balancer, CDN. And then in the end we end up at some applications that we write as application developers. So if we're building a TV streaming service, we might end up with lots and lots of different services. You know, orth services, progress, bookmarks, the thing which starts the playout, etc. And today we're going to drill down into just one of those. We need to keep the wider system in mind, but we're going to focus on one application in a larger system and how to architect it so we can continuously deploy it. We all like boxes. Everything's a box if you think about it at a high enough abstraction level. But we need but that box once we zoom in is going to include lots of different things. So the bits I'm going to cover today is the ingress at the application layer. So I'm not talking about the the previous step where it's like you know your global load balancing. It's how do you route requests or get messages off things like cues to end up at multiple instances of your application. I'm going to talk about different application hostings, things like serverless, things like things like uh containers, VMs, bare metal, etc. Um, and then finally the backing services that our applications use to actually, you know, achieve their behavior, things like caches, databases, etc. So, just to to summarize that, we're going to use two terms throughout the talk. We're going to use system architecture and application architecture. By system architecture, I see that as just a group of delivery units. Delivery units is a generic term I use to describe any of the four types of change. You know, infrastructure, vendor products, more data pipelines and applications. It's made up of the public API API of the system, how those delivery units interact, the infrastructure, the network topology, etc. And then we're going to have a look at something more focused on application architecture. And for that, it's going to be, you know, one thing. going to be its public interfaces rather than the systems public interfaces. How we use and interact with our backing services. What technologies and frameworks we use. How we engineer it for testability so that we can deploy it very quickly with a high level of confidence and how to structure the monitoring and observability of each of your applications so you can quickly promote it through a set of environments. But we always have to take a step back as software engineers even though we really like to focus and solve one problem. And that's what we're going to do today. You know, the real businesses that we work for only care about the how quickly you can deploy the full system and userfacing features. So, let's just try and keep that in mind. One of the key things which I always do when I'm thinking about continuous delivery is measure the continuous delivery at certain levels and especially if we're trying to make an improvement. So, and I do this at multiple multiple levels depending on who I'm communicating with. So when we're thinking about continuous delivery, I always try and define a top level metric that we're trying to improve, right? So it's, you know, I need to release two features per week and we now userfacing features a week and right now we're only releasing one per month, right? So that's the top level thing. However, what we're going to drill down to is things like how to reduce the lead time of one application. So that's the time it takes to get from commit all the way to production and how the application architecture affects that. You know, these are all the types of things that I would want to measure during a continuous delivery transformation. And we're just going to drill down into application architecture, but we always need to keep in mind that we need to we actually need to change the thing which the business cares about. We don't just get to play around with one application. So this is the ever growing list of things. I think when I um first created like this deck, it had four things on, then it had five, then it had six, and I didn't want to get to seven. So I started indexing from zero. Um so we're going to go through we're going to start with interfaces. So how does the interface of your application um affect whether you can continuously deploy it. We're going to talk about disposability and how like when you shut down your application, you're not ever shutting down your application. Quite a common um well understood one now is statelessness. I'll cover it because I couldn't not but I think it'll be very brief. We'll talk about externalizing observability and how to include all of your observability um resources versioned with your application so that that can be promoted through environments. I'm going to talk about the importance of decoupling from your underlying infrastructure uh and how that can speed up your um delivery. We'll talk very briefly about database schemas and the management of how we promote those through a set of environments. And then throughout I'll talk about testability because I do kind of think the only way to deliver something fast is to have a high level of confidence that your service still works and for that you you you need you need you need tests. These are all I technical pillars I'd say of architecture for continuous delivery. Many of the blockers for continuous delivery organizations are more people organizational how you structure your department etc. I'm not going to go into that at all today. It's all just going to be the technical sides of how how you structure your application. So let's jump into it. The first one is interfaces. So I think you have to think very carefully about the interfaces you expose to other delivery units or applications. Um because each of those are a point at which you're you you can break as you deploy things. So if we think about a very simple example so you understand what I mean by interface. I don't mean programming interfaces. is I mean like things like HTTP or gRPC or messages on cues. So if we've got two microservices or two applications that communicate with each other, you know, that is a very clear interface that we need to think think about. If one of our applications is actually exposed all the way through to the client applications, that obviously is an interface. Then we've got backing services. So that's the the terminology we use just to describe things like databases, caches, cues. Those are not public interfaces, I hope. Um so you might be using a relational database to store something like a materialized view. You might be using some kind of topic or queue or event store to you know build up your internal state etc. So I'm not considering those to be intern those to be those to be public interfaces. But you then might use the same technology to provide a public interface for your service. You know, so for instance, if we wanted to replace the HTTP call out from one microser to another, from the playout service to the audit service and instead publish an event, then that definitely does become a public interface of our service. Um, we can accidentally introduce public interfaces to things that we shouldn't. So if you want to quickly, you know, build a feature and in the audit service and you quickly query into the playout services database, you know, we shouldn't do that. But what we've done is turn an internal implementation detail of the playout service into a public interface which is not very good if you want to continuously deploy your applications because now you have to worry about the the schema of your database being a public interface. So just to summarize that um if I'm looking at any applications and I'm evaluating whether they're going to do continuous delivery or I'm building a new application I always make like a point to like list the public interfaces of that applications. We've seen they could be HTTP APIs, gRPC messages consumed, you know, off a topic, etc. And if you really want, they could be shared database tables, but I don't recommend it for each of those public interfaces of my applications, but also the applications that I consume because obviously that I go through their public interface. I'd want to see like very clearly documented compatibility guarantees. One of the key ways to be able to continuously deploy each of the components in your system is that the APIs between them, the public interfaces are both backward and forward compatible. So you don't need to coordinate releases. If we want to have the capability to actually make a breaking change, then we need clear versioning. And if we have clear versioning of those public interfaces, you know, then we need a a way to deprecate because we don't want to maintain lots of different versions of any of any particular interface. I'm going to be running through these quite quickly because I've got quite a lot but you know if anyone wants to come to me and talk to any of these in more detail after I'm here for the rest of the conference. Um the next one is disposability. I started my career a long long time ago at IBM and I used to uh work on a product which used to deploy to a mainframe and I was speaking to a bank in America and I very naively said all you've got to do is restart your application to which they said I had not restarted my application for 12 years and I don't intend to do it today but most of us do not live in this world anymore we restart we our applications are rebooted on infrastructure all the time so this is this is not the case um so we need to architect our applications to say that that so that you know when they shut down they don't actually shut down which sounds a little bit silly but I think the underlying infrastructure that we all typically deploy our applications to now does affect our application architecture. So I'm going to quickly just go through the most common types of application hosting for you know the applications that I work on. So I used to spend a lot of time building directly on top of uh bare metal. I don't really do that anymore. I know there are still use cases and like high performance computing and trading systems. I don't work on those systems anymore. We've of course got VMs and whereas I don't deploy any new applications on VMs, there are, you know, still motivations for putting things like backing services on VMs. However, I do spend a lot of time migrating applications from VMs and those are the architectures that I'm often looking at. By far the most common and and maybe this is the same for you are hosting infrastructure that I work on is you know our application packaged as a container and then there's some orchestrator or custom platform built on top of something like Kubernetes and then serverless and I think the architectural principles that we're going to talk about are pretty much applicable the same for whether you're deploying to a custom say Kubernetes in your organization or whether it's serverless I think you know many of the same thing many of the same things apply So let's go back to the point that you know the application when it's shutting down it's never actually shutting down. So nearly all of the application hosting infrastructure that I work with will support something like a rolling deployment. You know if you want to be able to deploy your application a lot you can't have downtime in most scenarios. Um so let's just quickly run through that. So let's say we have version you know 1.1 of our playout service etc. When we come to deploy the next version we'd create an instance. we then shut down another instance and then we cycle through. So that's what I mean by most the time when your application is shutting down, it's not actually fully shutting down. So we have to do it very carefully. Um, one thing that's we're going to go into later that becomes very obvious when you show a diagram like this is if we want to continuously deploy our applications, then we need to handle and architect our applications to handle mixed versions. So that is multiple versions of your application running at the same time and all of the implications that's going to have on how you interact with your backing services like your databases, your databases, etc. Rolling redeployments is not the only reason why your application might be shutting down all the time. There's also things like autoscaling. Um so if you don't have this, it this is something where you know the resources given to your application is going to change over time. So imagine, you know, we're 80% CPU. The application hosting infrastructure can create another instance for you, but then when the CPU goes down, it's going to shut it down. Both of those examples you you know might might be obvious because things are happening to your application. We're either doing the deployment or the load is increasing, but there's a whole host of other examples. So if you imagine the the underlying infrastructure that runs your application, it could be the cloud provider doing it or it could be your platform team, you know, that that runs your your your container orchestrator. Well, they constantly need to replace and add and remove nodes for patching, etc. So what do they do? Well, they typically add the node. They then drain another node and they slowly move your application over to the other node without you doing anything. You haven't triggered your pipeline here, etc. Your application is just moving. So we need to architecture our applications to be fine with that. So what do we mean then? So what do we have to do? Well, there's really three things that we have to do with our application. We have to define multiple types of health checks so that we behave well when these things happen. We need to gracefully shut down our application. Paying particular attention to the requests that are currently ongoing when that's happening. And then of course we need to to to to start up fast. We'll start from the bottom and talk about health checks. So whatever internal load balancing you're using um it probably has the concept of livveness and readiness these are terminology which kubernetes uses but you know I think all types of load balances will have some some similar concept and it's important important for you as a developer to understand them and then implement them in your application. So the first is livveness. How do we know is your application running? Is it because just the process is running? Is a port open etc. And more importantly, how does my application hosting know that my service is ready to serve user requests? And again, I'll go into a little example of why having both of these is very important. We'll use the rolling deployment example, but there are many but any of the others could could have done this. So imagine that your service is architected in such a way that even though it starts up very quickly, when it first starts up, it serves requests a bit slower. It could be because you need to do a computation, load some data from a cache, talk to a database. It could be a slow JVM start any any reason. It doesn't provide a particularly good user experience if suddenly of a quarter of your requests suddenly take 2 seconds rather than 2 milliseconds. Especially if our wider system is actually made up of lots of different services and you know one user request is actually going around them all. If we continue with the rolling redeployment it gets even worse because now it's not a quarter of your traffic, it's a third of your traffic assuming that we're just roundroining between the various instances. So that's where the concept of readiness comes in. Um, so instead of um, uh, starting with the rolling redeployment right away, when you first start, you would tell the application hosting infrastructure that you're not ready to serve requests. This could either be by a command or a typically a HTTP endpoint that you provide. And then once you have done whatever work that you need to do on the startup of your application, you know, then you go then you tell the application hosting infrastructure that you're ready and then it will continue with it. So that is the the first pillar of of disposability. The second one is graceful shutdown. I learned this the hard way many years ago that um this is this you it's often very hard to gracefully shut down your application. So nearly all application hosting infrastructure before they shut you down they won't just kill you. They'll send you a sig term. This is what things like kubernetes do. It's then your job to gracefully shut down. And if you don't do it within a certain amount of time, you're just going to get killed and whatever requests are going into your application, they're just going to get they're just going to get lost and your users are going to get, you know, whatever the load balancer decides to return to them. So when you get that sig term and it's important that you test that you actually get the sig term and I'll explain why later you need to stop accepting connections you know or the equivalent if you're in more of a event-based architecture you need to stop consuming messages off the queue or however work comes into your application and if you've implemented the previous step you should also mark yourself not as ready then you should have a way to um know how many concurrent requests that you've actually got inside your application so this would be messages you've taken off AQ without uh without acknowledging or if you had long lived say TCP connections coming into your application it would be the it would be the requests that are currently in flight um you could fall back to some type of timeout but in reality if you had it in your application you know that you know when there's a when there's concurrent requests then you can actually shut down at the right time before you actually do shut down then you can close any connections etc so you shouldn't really rely on the error handling of your backing services, you know, things like databases, flushing metrics, etc. So, once you've completed the requests, you then shut down. And then you've got to actually remember to shut down, right? Because you the goal is that you should you should exit as quickly as possible without losing any of the in-flight requests and you should exit with a zero status code. If you wanted to go and do like a little checklist for your applications, this is the checklist I go through. So, the first thing I'd do, assuming you're in a container, is make sure your container actually handles the signals. Often we build containers where our process, our Java process isn't the first process in that container and then that SIG term doesn't even get to our application and then we just wait for the orchestrator to hang out. So check that works. If you're using a framework like Spring Boot or whatever framework, make sure see if it's got any single signal handling in many of them do now and they've often you just need to you just need to need to configure them. Um test that that signal handling works in a real environment. what works on your laptop might not work in a real like in in your real environment. And then the absolute bonus point is if you can have an automated test in your pipeline which does this. And I'm going to talk about testing strategies for continuous delivery a little bit later. But it's fairly easy to simulate like a slow running request, trigger a rolling redeployment and make sure all of those requests are are successfully completed. And I find things like this stop you from having, you know, in quotes blips. You know when people say oh that we had a few failures but we couldn't um diagnose them you know it could have been because your application was getting moved between nodes during a maintenance event you know during your application hosting infrastructure but because we didn't shut down we didn't shut down with the sequence etc we lost the the the inflight requests the next one is to be stateless which really is just building block on making your application um disposable I think being stateless is a pretty common a very common pra practice now but I just want I this talk topic could not not have it in. Um so let's just uh introduce some load balancing terminology. So you know how we we we pretty much always when we're doing continuous delivery run multiple instances of our application and we need to think about how the requests are getting there. So the most common is just around robin you know that's where we just send requests to you know instance A then B then C. We've got slightly fancier things like heruristics. So if we want to load balance more equally across them, we could say the one with the least number of connections gets the next request. We've then got sticky sessions. This is we typically only need this if we've got a stateful application. That's where we can use something like a HTTP header to ensure that you know a request for a particular user always ends up at the same instance. But that's what we want to avoid if we want to be stateless. And then we have things like weighted distributions. So if we're implementing things like canary deployments which I'll chat about later um we can route certain percentage of certain requests to certain to certain instances. So I worked very hard on this animation so I hope you appreciate it. Um um so imagine a request comes in to your load balancer etc. It gets rooted to one of your applications and this is a very well-known orange customer. Um if we are stateless then that would be the end of it and that piece of orange would go away. But if we decide to store that somewhere i.e. on the file system of the of of the application or we store it in memory etc then we become a stateful application. Imagine that's like the logged in user session state etc. It means then that future requests would have to get would have to get rooted to this instance. Um but you know requests coming in for different requests could go to different instances. But then we've already learned that many times our applications are going to get shut down and our poor old orange customer now has lost whatever state they were doing etc. Um the the obvious and well-known answer is always to externalize your state. It could be a cache, it could be a database, etc. Um there's absolutely nothing stopping us from optimizing by bringing things into memory etc. as long as it doesn't actually affect the functionality. So that's what we mean by being state by being um stateless. not really architecture or maybe it isn't is architecture and it wasn't in my lists so I definitely didn't want to go to eight was the concept of delivery unit encapsulation I'll just call this application or service encapsulation and this is the importance of putting everything that you need to deploy your application not just deploy it sorry deploy it run it monitor it in the same place and my strong preference you know is a is is a is a repository because I think to enable continuous delivery. Dependencies are the killer. They could be technical dependencies like you know coordinating the deployment of this application or this application. They could be people dependencies like I have to get the QA team to to work on the change next etc. So what I really want for an application is to have the code. That's the obvious one. We all hope that we the the code for our applications are in the same repository though I've seen many instances where it's actually spread about but you know please don't do that. the tests. Now, we're pretty used to putting the functional, sorry, the unit tests in with our code, but I also think all of the functional tests, performance tests, the synthetic load tests for production, if you have them, you know, those should also be in the same repository and versioned with a with with your application. Again, less controversial, but not that common when I when I start to work with new people. Um, the next one is the monitoring and alerting. So, so I'm talking about things like the obviously your application publishes metrics, it has log patterns, etc. But you use those to to to to run your application in production. So, we need things like dashboards. We need things like alerts. I would also have those in the same place and versioned the same way because they really are coupled to your to your metrics and your and and your code. And then we're going to talk about configuration later because I think configuration is the one of the hardest topics in in in continuous delivery of applications. I like the term diva. I used to call this I used to spell this a different way like immutable deployable versioned artifact but div.va sounds better. So once we've got all of that stuff in one place say a a repository. I feel like the next thing you have to do is turn it into something immutable and that's deployable typically this is an OCI image right which we then run into like we either then send off to a service off or or a container orchestrator. Um, you could put everything inside one diva or you could have it separated. So you have a version for your tests and a version for your application, but they would have to be the same version and promote it together. So as I move my application through environments as part of my continuous delivery process, I've got a version of the tests at the same time. I'm checking those things at the in at the same time. And it's all part of the culture of trying to get people to build services, not artifacts. Even though this is an artifact, it's got the word artifact in its name, it's deployable. The first thing we're going to do in our continuous delivery pipeline is deploy this and make it a service. So let's go through these one at a time. So testability. So what what type of tests are we doing? Yesterday I went through a full path to production contract that I've used at lots of different organizations and lots of different jobs. Um today, um this is a picture of just part of it and it is the fast feedback bit. something you might call continuous inte some people would call continuous integration but it shows three the three types of tests which I think we need for continuous delivery uh the main one which I'll talk in a bit more detail than I did yesterday is stubbed functional testing and these are the tests which I'd want versioned with the application and they test all of the component they test my component in isolation without worrying about the other applications so if we go back to this like very simple application that I put at the start and we leave in both types of public interface. So, it's the, you know, the audit service having a HTTP interface and the playout service publishing events. I just left them both there so I could talk about how we could test it in both scenarios. So, we want to build a high level of confidence in in in in our applications as we continuously deploy them. For me, the best way to do that is to have a high level of test that can be run in a very short place of time. So I want to architect my application to clearly specify which other applications I depend on and which of their interfaces I depend on. Uh and this is basically how I go about doing it. So the first thing is I take that diva that deployable immutable versioned artifact and put it into a realistic environment. If we don't have a realistic environment then we work with our platform or infrastructure teams to make sure that we can have realistic environments for our for for our testing. I deploy it with real backing services. So when I say stubbing, I do not mean stubbing out things like my database, etc. I might run a much cheaper version. So rather than running my fullyfledged, you know, cloud SQL, highly available, backed up, multi- region, whatever, I might just run, you know, Postgress, say, as a container. Um, but it's a but I run it with that cuz as far as I'm concerned, it's just part of my application, part of my application architecture. That said, what I don't do is deploy any of the other applications in the system um to do to do this test. They use things like stubs and I'll go into what a stub is now. So stubs are real things. They expose the actual protocol and they offer a functionality called priming which means that I can tell them to pretend to be other things. Right? So I'm going to say when I call you on this API respond with this and we'll talk about how we can do fancier things like f failure scenarios you know then I interact with my service and as far as my service is concerned my application it thinks it's talking to the real thing but really it's talking to a stub and then I can talk to the stub and verify that the right things have happened. So when I'm thinking about my application, the thing I'm testing is its public interface and its interactions with the public interfaces of other delivery units and nothing else. When the public API or the things you're communicating with is an event, it's it's it's easier really because if you think about it, you've got your playout service, you've got your interact, you've got your test, it triggers it rather than a call out to an external service like a gc, HTTP, you know, it's putting the message on the queue and then you can just read it off and verify its contents. There's a lot less flexibility on being able to prime, i.e. like prime failures, slow responses, all those types of things. But there's also lots of benefits in this type of communication between applications. So, you know, you win some, you lose some. So, the key stuff being functionality that I want to rely on is being able to prime. I think priming failures is how is is is extremely important. So, if we've got lots of different applications and they all communicate with each other, we want to know how our service behaves when our dependencies are failing or going slow, etc. Because then we can have things like circuit breakers, timeouts, and we can test those things. um because we can we can make our stub pretend to do those naughty things. Um and this is the point where you could test that graceful shutdown. So you could imagine that the playout service in the previous example is calling the audit service. You can prime the audit service to take 10 minutes to respond. Put in a a request shut down the playout service and you can verify that that request actually completes assuming your timeout wanted to be more than more than 10 minutes. So that's how you can actually test things like the graceful the graceful ter um termination of your application. And then of course the more details that the stub can give you about the interactions the better because then you can write tests more tests around how um the um you know the service calls the stub. I also use these stubs for doing um what's called stub non-functional testing. So that's where I would run a performance test against my application. And of course if you're loading that application then that application typically loads others etc. So I can verify I can verify things at scale. Um I highly recommend YMOK. I've used I've used Ymok for over 10 years I think probably longer. It's a great HTTP stub there. There are other tools about if you Google this type of stuff. So I don't have you can you can use whichever you like to kind of like summarize the testing principles of continuous delivery. Then so isolated verification architecture application. So it's really decoupled from the other applications in your architecture and do 95% of your testing as these stubb stubbed functional tests and you can gain a lot of a lot of a lot of um confidence in a very short amount of time. Make sure that the tests are extremely loosely coupled. Um so not just your architecture needs to be loosely coupled but show should your tests. I think a really good test of this is imagine you were going to rewrite a service. You're going to throw it away. We're going to get rid of all the Java and we're going to rewrite everything as Golang, right? Assuming you weren't actually changing the behavior of your application, could you use the same same test suite? If the answer is yes, then you know you're writing loosely coupled. You're writing loosely coupled tests. Always try and test like the re as much of the real thing as possible. So I don't mean integrated with other applications in your architecture, but test in a real environment with everything, the configuration, the application manifests, etc. And I think you have to architect these things to be fast, right? I never want to spend more than 15 minutes from the point I either raise a pull request or merge a pull request into main to get extensive feedback about whether that change is likely to get into production. And I think all of this really relies on having, you know, very clear definitions of what a backing service is and whether it's an internal implementation detail of your service. You know, very well doumented uh uses of public interfaces and then lightweight environments so you can actually run these types of tests uh realistically. The next thing is observability. So how should you architect your observability for continuous delivery? I'll talk about two things which I think are pretty much a given now and then two less commonly adopted things. So um I alluded to this at the start but I feel like um but I'll say but I'll say it again is things like dashboards and alerting are part of your application architecture and I think it's very important that you build them as part of building features and you merge them together and you verge them together. So we go back to this encapsulated delivery unit, you know, this repository which contains everything and we talk about what happens. Well, typically we're going to build it into an OCI image that we're going to deploy to our application hosting. Now the other types of things that you're going to rely on say platform services are things like aggregated logs. So to decouple yourself from the underlying infrastructure obviously the most common way of doing that is that logs go to standard out. you're not directly integrating with any particular aggregated logs provider and then something else is shi is shipping those and pretty much the same is the case for metrics these days and there's things like open telemetry or you can expose like uh your metrics on a HTTP endpoint that could be scraped into Prometheus or data dog etc but your application should not know about that it just exposes metrics and they get somewhere um to make the things I've said before about dashboards and alerts uh more concrete well I would actually have my dashboards as code inside that repository. I would have the alert as code inside that repository and as part of the deployment to my application hosting where I'm deploying say you know say I'm deploying to Kubernetes which would mean some manifests and a container image I would also be deploying to my hosted dashboards and hosted alerting. So this goes into not just application architecture but how do I select other technology architecture because if I'm going to use a particular observability provider the first thing I'm going to check is can I create everything through an API so I can deploy to it like from my delivery pipeline etc and is there some concept of versioning right so I'm not just like replacing a dashboard breaking it and then I can't uh roll back a common question I get then is what happens if my company has already decided on a certain and like you know dashboard metrics aggregated logs and alerting provider and they don't have this functionality that's where I tried to be pragmatic if you've got application hosting which supports containers and perhaps a persistent disk then what I would do is actually put a lot of my all a lot of my things like dashboards and alerting I would just deploy myself a Prometheus a graphana etc I'd have all my high granularity metrics there and then I just export the key metrics the business needs into their their solution if it didn't support the thing I needed a really nice quote which I often go back to um from 12 factor apps is you know have a clean contract with the underlying operating system offering maximum portability between environments and you know this is what I think the modern version of this is right as your application the only thing you compare you you you really depend on is you expose your logs on standard out and you expose your metrics on um some HTTP endpoint and then perhaps you open up a port or you connect to your backing services through a port so we don't need to couple ourselves um to the underlying infrastructure. Let's talk about the types of metrics. Um and there's the types I hope that you just get for free and then there's the types that you as an application developer have to build in build into your system. So we've got very technical and delivery oriented metrics. I'm putting these on here even though I hope you just get them from your underlying application hosting infrastructure because one thing I would do is if I don't get them for free, I would produce them because I find them very useful to be able to, you know, monitor and run my system. So we've got things like technical metrics, you know, C CPU, memory, error codes for common protocols like GCP or HTTP, etc. Obviously, you can expose these yourself from your application, but perhaps your underlying infrastructure does it like through a service mesh or you know things like infrastructure monitoring delivery metrics. So depending on how you actually deploy your application to production, do you have centralized pipelines etc. I would want to know things like deployment success and roll back frequency and you know lead time etc. Again you can build this into your own custom pipeline if you don't have some central function for this. Um but ideally you get these from your your from a pattern production say from your your CD team your platform team or whatever your organization calls it. The thing that we have to put in our application and no one can do for us is our business metrics. So every application that we build we should look what user journeys are actually is is is my application responsible for and then I should expose very key metrics that I can use later to know whether my application is healthy and I'm going to talk about architecting our application for canary deployments later and it's these are the metrics that I would use to decide whether to promote a change from a canary deployment to a to a full production deployment. Next topic configuration. Who here likes configuration? I hate configuration. I I just wish I just wish all of our applications could never have any configuration. About 12 months ago, I was helping um a team migrate some applications from some legacy infrastructure onto like a modern developer platform. I got to the first service. It had 114 different bits of configuration and it had 12 environments and it was completely different configuration per environment and there was zero confidence that the testing done in any of those 12 environments actually replicated to production and then of course you had like you know the configuration guru at the organization that would just like see a customer problem and tweak the 114 things in production and I think when we finished we had four bits of configuration and two of them were two of them were passwords I um which couldn't be done with something like workload identity. One of them was like an environment prefix etc. and one was whether autoscaling was enabled. So configuration I think my first piece of advice is do not make your applications configurable right the majority of configuration is completely like pointless or needless etc. I used to work at a trading company where we released once a week. It was a very expensive thing that we did at 10 p.m. on a on a Friday on a Friday night etc. But we were allowed to tweak the configuration in production. So we would make everything configuration because change was hard. Change isn't hard now or at least we have the ability to make it easy etc. So I'm completely okay with running my full path to production just to make a configuration change. We've got environment specific configuration. My advice here is very similar like minimize minimize minimize etc. One of the things I like to do is go through an application, look at the configuration, see why it's different per environment and then play the game of like does it really need to be can it just be like an environment name prefix etc that's used to build thing like things like URLs etc and for me continuous delivery is all about building confidence as you promote change through environment so the last thing I want is for the thing to be configured different differently we've then got sensitive configuration same advice minimize minimize minimize like if you can use workload identity to connect to things and IM etc. like just don't have the sensitive configuration got to be pragmatic though you're probably going to have Salmon we should integrate with some kind of secure store and have some kind of schema for like how certain secrets get into certain to certain environments and in reality most sensitive configuration is environment specific configuration just to reference my my my my favorite website like 12 factor apps etc. So you know they go about and say you know store configuration in the environment and configuration really just is the thing which is different per deploys. So things like staging and production etc. I quite like the terminology that they use for separating build release and run you know. So your code is built you combine it with configuration to turn it into release and then you deploy it into an environment and then it can take in environment specific things from the environment. I kind of I've changed my mind on this a lot over the last 10 years or so, but I would just see now this is our delivery unit encapsulation and I would typically keep profiles a very small number of profiles of configuration just in the repository etc. because I used to be optimizing not for change um now I'm I optimize for change. So if I need to change those etc. Um I'm very happy just to run the full pipeline have a commit to the repository. And then the question that always comes up next is like what happens if you want to spin up a new environment and I quickly say I don't want to spin up another environment. One of the key principles for like adopting continuous delivery I think is to have a welldefined set of environments which your change goes through. If you need things like developer preview environments that's totally fine but the only configuration parameter I'd have for that is the um like environment prefix. So I can change things like URLs etc. And you know another quote from 12 factor apps is like minimize divergence between deployment and production or any environments that you use on on your continuous um pipeline etc. Um so that's it. Yeah have as little configuration as possible. One of the hardest bits of configuration to deal with of course is your database schema assuming you can you consider it to be a bit of configuration etc. So if I quickly run through and this would be you know similar for other other technologies like what a database is. A database is some infrastructure that it runs on. It is the database and it is obviously the schema that your application is very tightly tightly coupled to. I would have the infrastructure and possibly the database as completely different life cycles i.e. provisioning VMs or or creating the you know the the managed service etc. But the schema absolutely has to be as code and versioned as part of that encapsulated delivery unit etc. There was a whole talk on this yesterday for 50 minutes. So go and watch that for the details. I would pick one of the very popular tools and become an expert at it in your configuration. My background is definitely more with Flyway, but that last migration project was Atlas. It seems fine. If you need a language agnostic tool, it seems like a very good one to do. I would highly recommend minimizing the backing service technologies inside your organization. It's always very exciting to use new use technologies, etc. But they all have their own idiosyncrasies with what's easy and what's hard and what's expensive when you come to make things like schema changes. And you do not want to have to keep more than one in your head. You know, pick a pick a popular database. If you need another specialist database, say Apache Cassandra for multi-reion deployments, you know, maybe have two. And then, you know, obviously look into the architectural patterns for when you have to make a breaking change. Things like expand, migrate, contract. But also understand that you know these things are expensive to implement if you're like you know releasing multiple versions of like tables and double writing and migrating etc. But the the main takeaway uh cuz this is only one minute of a longer talk is like the schema has to be versioned with the application deployed with the with the application at the same time and it's a really important part of delivery unit encapsulation. The next uh topic is designing for reduced blast radius. Um, if I think about everything that I've talked about so far, um, it's about the delivering with confidence and testing and it protects you from the known. So, if you've thought of something that could go wrong, we're all good at thinking of things that could go wrong. You write a test for it. You know, it's not going to happen as you promote a version through. Whereas like promotion and especially gradual roll out, you know, that really protects us from the unknowns, the things that we're not expecting to happen. I hope as application developers you never need to look at a diagram like this or really understand the underlying infrastructure for your um application because you've got a real good got a very good separation but most infrastructure environments look a little bit like this. You're typically across multiple regions if you're if you're running at a large enterprise um there's a load balancer a top it and you've got local load balancing and your application is typically deployed across multiple zones etc. So those replicas are spread out and I implore you to ask some questions about your application and your application architecture. So can your application handle multiple versions running at the same time? Um a more specific version of that question is can a user journey switch between a new and an old version. So for instance, if a user comes in and goes into one of the new versions because you're doing a rolling redeployment, what happens if a bit of data goes back to the user and then comes into an old version of your application? And do you have functional tests for that? So can you actually run a functional test with a mixed version because it's going to happen quite a lot. And then the next one is does your persistence strategy allow you to actually have your application active active across regions. It makes your persistence strategy much harder but it makes your application deployment and application architecture much easier. And one of the funny questions um I like to ask is is your application availability so the uptime of your application increased or decreased by deploying across regions. I often work at organizations where they mandate you deploy across regions. Perhaps it's you know and something inferred from a reg from a regulation etc. But many application architectures actually reduce availability once they deploy across multiple regions. And the prime example is they rely on some transaction in the database that suddenly is only available if the replicas of the database are running in both in both regions. So actually one region outage can actually take down your application whereas previously it could had a 50% chance of your application being taken down. All right, I'm slowly running out of time. 3 minutes left. So I'm going to quickly um do the case for decoupling your application architecture from your infrastructure. I'm going to go back to my favorite quote or from 12 factor apps of having a very clean contract with your underlying operating system. I'll modernize that a little bit and say you should have a very clean contract between the underlying cloud. Who here has deployed a container? Anyone? Everyone's deployed a container. How many different ways is there deployed container on AWS and GCP? I have no idea. And I've I'm I'm old and I've worked on so many different systems and I've worked the last five years as um more of a consultant. So I see lots of people's architectures it is endless you know there are so many different ways and it's constantly changing and you know offerings are being deprecated etc. you know the application architecture which kind of like you know lasts the test of time is this right and it's very much from 12 factor apps you should communicate to your backing services over the network you should externalize your monitoring you should either take messages over Q over the database or you should accept requests etc most organizations have a huge number of software engineers and a very small number of platform engineers very happy for the platform and infrastructure engineers to have a very busy week if you decide to change your underlying say container you know hosting or you decide to change cloud provider, but you don't want it to affect all of your application developers. I've got three different case studies um for very different scales which like could interest you and kind of back this up. So there's one case where we helped build a a developer platform on one cloud provider. We agreed this principle up front that we weren't going to couple the application developers to the underlying cloud because there were over a thousand of them etc. And then 5 years later we added GCP support to the underlying platform and then the applications could just flip over which was used as like a big commercial lever between cloud providers etc. And it was because their application looked like this right so the database in that question was Apache Cassandra which handles multi-reion even across cloud providers very well. That's a very big case. I'm not expecting everyone to kind of like resonate with that one. A much smaller one was a project I worked on where we'd built a very high throughput application. Um, and it was on cloud run which is Google one of Google's containers as a service. We actually hit the performance bottlenecks. We even had Google come in and tell us that the latency and throughput that we wanted wasn't possible on this offering and they told us we had to move to a GKE based offering and host the container there. But because the application architecture looked like that picture, the application could be switched over right away and it was just a bit of infrastructure provisioning from the more platform focused people. Another one which I take no credit for at all was we um helped a team uh a whole company migrate cloud provider for some we for reason I won't go into they had already independently architected their application like that in a few days we had the application running from EKS onto a GCP based platform etc no application code changes um so I think decoupling application architecture from infrastructure it's often like described as like overarchitecting you know there's no point etc But I think really it just requires you to agree and then I don't really think that it increases scope. 3 seconds left. So closing up resources. If you haven't read the 12 factor app website, please go and read it. The original continuous delivery book is fantastic. There's a QR code for some articles on um on mine our website. Feel free to go and read those, etc. They'll all kind of like they'll probably they're they're at a bit of a a higher level, etc. The other thing I'm planning on doing is releasing like a free tool for evaluating your architecture for continuous delivery. It'll look something like this. And then it will like show you based on your components which ones are the most continuous delivery ready. Don't worry if you put your email in on that QR code. You'll just get an email about that. You're not going to get any other other emails. I promise. Uh on that note, I just want to thank you for listening etc. If you want to talk about this more, you can connect with me on LinkedIn. Come and talk to me now. Thank you for taking the time to listening to the talk. I hope it was useful. [Applause] [Music]