Hello everyone. Hi. We're going to talk about everyone's favorite and least favorite language, JavaScript. Yeah, arguably one of the most important languages ever made, which is a sad statement. So, we're going to have to make it really fast because it runs everywhere whether it should or shouldn't. Uh, my name is Eric. This is NDC as you know. Uh, by day I make educational software and by night I make video games. So, I learned a few things about JavaScript that way. So, I maintain an open source library called Excalibur.js. It's the 2D TypeScript game engine for the web. Uh, you can make video games in Typescript. Check it out. I am the co-host of Typescript.fm podcast. Um, you can get to know me on Blue Sky and I have a YouTube video where I talk about gamedev stuff. So, if you're like, "Oh, who is this cat up here?" He's like, "Oh, he does video games in JavaScript." So, I picked up a few things about making JavaScript fast. Um, here's some links if you were like, "Oh, I don't remember those." I'll just pause there for two seconds. All right, cool. All right. So, what are we going to talk about? Uh, we're going to talk about why JavaScript. Because arguably, if you're like, I want the most performant thing on Earth, you probably wouldn't pick JavaScript. But we have JavaScript. Um, there's some secrets. I'm going to orient this talk. I'm going to frontload it with the highest impact things first, and then we're going to squeeze blood from the orange or from the rock, depending on which metaphor you're going for. uh and get into the weeds a little bit and see what we can do. So, we're gonna start with your Wow. We're gonna start with the biggest bang for buck. I'm going to stand over here. Biggest bang for buck. And then we're going to move to maybe the more esoteric. Um we're going to talk about how JavaScript actually works because you kind of need to know how it works in order to make it fast. Uh some techniques, some optimization strategies. I'm going to focus mostly on making CPU fast, making memory fast, and how to write code that the JIT likes, and a few final thoughts. All right. Can JavaScript be fast? It can. It can. Why do we care? JavaScript won. I'm sorry. I'm sorry, everyone else out there. JavaScript actually won. It It runs everywhere whether it should or shouldn't. actual human dollars is why we should care. Apparently, slow websites cost money. And according to this article, 4 million in revenues per millisecond. Yikes. Here's another one. Yikes. And you can see a number of these articles online. Your users might not like you if your JavaScript is slow. Um, if your JavaScript is eating through CPU, it's eating through battery. And battery is not free. Uh maybe you're building a real-time application like you're building a chat app, you're building a game, you're building a simulation, you're doing something in finance, you're building a digital audio workstation, you're doing like actual like video audio work, you need to be real time. There is some bad news though. Um Moore's law has kind of ended and single core performance isn't going to get much better and JavaScript is mostly single threaded, mostly asterisk. We'll talk about that. So, there are a few dirty secrets to performance. You just got to do less. You got to do less stuff. There's no like magical fairy dust. It's just you got to do less. So, that's the bummer there. There's a lot of clever ways to do less stuff. And that's what this talk is about is clever ways to do less things. Um, performance optimization often makes code harder to read and have insidious bugs. So, that sucks. And the other thing that you need to think about when you're doing performance optimization, is this even worth doing? Like, is it good enough? Like, that is a perfectly okay answer. You don't need the fastest JavaScript all the time. It might be good enough. Okay, here's my very scientific graph based on my own personal experience. um as performance goes up, you get some gains right in the beginning, right? You get lots of gains right away and then to get any more, it's kind of like this diminishing returns, but as you do that, maintainability drops to the floor and bugs go exponentially. Um, so there's kind of a sweet spot there in that triple point where you can kind of make it all work and you won't be sad. All right, here's my rules. Should you even do this? So say you have 10% of your app um and it's really slow and then you have 90% of your app and it's moderately slow. You should optimize that 90%. Don't focus on the 10%. If your app is spending 90% somewhere, focus on that. Set a goal. You can do this forever. Trust me, I have. I've been like stuck in the hole. Keep digging and like it's it's not fast enough. Just set a goal. Like what is good enough? Like make your product person pinky swear to you what is good enough. Like make them tell you a number like for real. Make them say a number out loud. Write it down and be like it will be this fast. Measure first. Where are you starting so you know where you're going? Optimize the biggest fish. So after you measure, what is the thing costing you the most? Do that. Don't like start noodling over here on for loops or something or you know the best way to add numbers. does. If that's not the problem, measure again. Did your thing work or not? Do science. Repeat steps three through five until you're at your goal. And and okay, so here's one thing. You need to beware of microbenchmarks. And I say that and I'm going to only show you microbenchmarks today. So what I mean by that is microbenchmarks are not your production code. your production code might manifest different performance uh characteristics than in the lab. So you need to be measuring in production with real data to know how your app is performing. Okay, so how does JavaScript work? Well, there's this script I think that comes over the network and then the browser gets it and parses it, makes an a that's all fine and dandy, but then it goes into an interpreter and all JavaScript engines do this. It goes into an interpreter and most of them produce a bite code and then that bite code depending on your JavaScript engine gets put into an optimizing compiler and then you get optimized code at the end. So there's a few ways to get speed in JavaScript. You know there's network and parsing. So like there's content delivery like you want to make sure that you're delivering your script efficiently. You want small script. You want things to be codeplit. There's all that good stuff. There's DOM interactions. Those are important for fast apps. I'm not going to talk about those. We're gonna talk about JavaScript, okay? No, no other stuff. We're just going to focus on JavaScript. There's a lot of meat on that bone. There's plenty of good stuff there, though. So, I don't want to leave you hanging though on the DOM. So, I know we're focusing on JavaScript, but there's a few things. Um, if you have a list of elements, like say you're designing a list box and it's like a list of students and you never thought that someone would make a class with over 40,000 students, you're you're wrong cuz someone will and it'll crash everything. Your browser will be very slow. So this is like maybe use a virtual list. So lots of elements in the page at once really really makes things tough. deeply nested structures. SVGs really hurt if they're complicated. Um, so a lot of times what you wind up doing is switching to like a canvas if you have a really complicated uh graphic that you need to draw. SVGs are great for more simple graphics, but as soon as they get super complicated, that can really eat into performance. Complex CSS selectors, you know, that star, use that very sparingly. Uh, layout thrashing. What this is is like I'm reading the DOM, so I'm like saying, "Oh, get me the bounding wreck. Oh, I'm gonna reset the width. Oh, I'm gonna read the bounding rack. I'm gonna reset the width. And you're gonna make the browser very unhappy. Images. Uh there's there's a lot of talks on how images can really bite you, but you know, I have one fun story. A client called me up and they're like, "Our website is really slow." And I pulled up the website and it was a whole hero video of a bee landing on a flower. I'm like, "I think it might be the video." Yeah. Don't do that. And then DOM manipulation is slow, especially if you're co causing reflow and recalculation of the layout. Okay, so now that we got past DOM, there's a lot of tools you can use and you probably use these, but in case you don't, there's Chrome Dev Tools, there's Firefox tools, there's Node tools, there's framework specific tools. So if you're like using Angular or React, there are browser plugins for you. All right, let's talk about Chrome. If you didn't know, there's a memory tab or a performance tab up at the top and you can click the little record button. What will that do? That will record and sample your JavaScript. And then you get this fancy little flame chart where you can go and inspect and see like, oh my goodness, I'm running at 16 uh what is it? 60 fps. So that's 16 milliseconds frame. And you can see, oh, this app's doing okay. It's not doing a whole lot. It's doing some pointer calculations. Awesome. That's the Chrome Dev Tools. Use this a lot. Just use this all the time. Next up, Firefox DevTools recently got awesome. Did you know this? They're awesome. So, I frequently bounce between the Chrome and the Firefox DevTools. This is the same app I'm profiling. But what's really cool about the Firefox dev tools is it will give you the hot path right away. It's super slick. So, you can get right to the meat, what's important. Or if you're doing Node, if you go to Chrome: Wackwack inspect, you can go and connect to your node process that you have running locally. So you can go and use the Chrome dev tools to debug your node code which for a long time you couldn't do which really sucked. You can also do custom JavaScript custom tooling. So you can do console profile you know do some things profile end. This is non-standard but this is useful if you're like having a tough spot and you're like I don't know how to catch this if this is working right. All right cool. So that's like the overview. Let's get to the brass tax. How do we make JavaScript faster? Uh, cheat. There's cheating. Um, spinners, flare, animations, interactive loading, all kinds of good stuff. Software is about tricking your users into thinking it's working. This is this is my favorite quote. It's from me. Video games are all of this. You know, it's like, wow, this is really performant. It's like, it's a it's a lie. It's not actually. So, so the first thing the first thing to do is to cheat. This is the lowest effort thing you can do is the appearance of performance. Take that product owner. All right. The next thing you can do is data structures. There there are a lot of data structures to do things that help you out. There's maps, there's sets, there's weak map, weep sets, which are super useful if you need to cache things, but you also would like them to be garbage collected sometime in the future. There's heaps if you need to keep track of a minimum or a maximum. There's caching and memorization. Speaking of caching, caching is awesome. Be lazy and do caching. That's another quote by me. Uh, did you know that doing nothing scales infinitely? So caching is pretty close to that. So I can I can do nothing like really fast. Like I can do nothing faster than I can do something. So computers can do that too. So here's an example of how we might use caching like in a pure JavaScript sense. Um, we're going to make a with caching decorator here. It takes in a function, wraps it in another function and uses a map to cache the arguments and return back a result. So classic Fibonacci, this is really contrived. I know this is a microbenchmark, but say you have classic Fibonacci that's recursive. Don't do this in production, but say you have recursive Fibonacci and you need to use it. Um, the first one is going to take a really really long time as soon as you start getting to the bigger numbers. So the only way to make it work quickly is with caching. So here's an example of how to use that. I'm using that decorator. It's decorating that function. And we can take a look at it without cache. If we do fib of 40, it takes two and a half seconds. With cache, it's like barely noticeable. Isn't that awesome? Caching. Use it. Okay, here's a big bang for buck. You remember how I said JavaScript was mostly single threaded? Turns out that was a lie. We have web workers now. You can multi-thread in JavaScript. Isn't that wild? So, you can also ask the browser for how many logical cores you have. Navigator.h hardware concurrency. So, you can spin up the number of workers that would work for your client. So, the best number of workers to have is the number of logical cores, you know, plus or minus whatever you're doing. So, this API is a little goofy. It was made before ESM. So you to import scripts you have to call this weird import scripts thing uh and then it like loads it into the global. Not great. You can also span workers inside of workers. So you can go ahead and make more and you can go you can go nuts. You can also terminate workers and it'll just end them right there whatever they were doing. All right. So there's some caveats with workers. To send data to a worker it needs to be cloned. So they're working in different context execution contexts for security reasons. So anything that crosses the boundary from the main JavaScript thread needs to be copied using the structured code algorithm into the new one. So one problem you can run into is if you are marshalling a lot of data across the boundary between your main JavaScript and your web workers is your serialization can eat into any performance gains you might get. So, it's important to think about what data you're sending. And then there's another thing called transferables which allow you to transfer ownership very rustlike actually transfer ownership of something in the main JavaScript thread to web workers. Once you've done that though, it's no longer valid in the main thread. So, if you access it, it's an error. So, let's take a look at how web workers look. So, we send messages to web workers with the post message. So, here's an example. You do worker equals new worker. And there's this kind of fancy pants URL syntax. You don't have to do this, but a lot of bundlers recommend you do because then it can do the relative script from the current script versus like from the root because if I were to load this up and do like workerjs, it would go from the root of my website. So that's this whole new URL import meta URL thing is doing. It's giving me that relative URL. Just some fancy pan stuff there. And then you just post message. You can post anything. So, I just made command run stuff. That could literally be anything. I just made that up. And then on the worker, you can attach a listener and get stuff back. Yay. Here's what the worker looks like. And I'm doing that import scripts thing. We'll look at that library code later. It's not that complicated. Um, but it's loading up a library. And then there's a global hanging off the web worker scope called on message. And you can just attach to that and do some stuff. So, I'm just getting some data. time and then I'm echoing back that I'm a good good worker and I'm calling a function that I don't see here called add that's in the library. So when you do that import scripts it just lands in the global like UMD kind of awful but that's what we got. So that's how you do post message and here's what it looks like. So when you have your browser tools open you'll notice on the left there I have that library script that worker script in that little workerjs little gear thing. Pretty cool. And you can go and set break points in your workers and stuff. The tooling is really great. And then you can see in my console I'm a good worker. Yay. Let's talk about transferables. They only work on certain things. Um, so go check MDN. This list is growing. Uh, but they need to implement the transferable interface in the browser. The one that I use the most is array buffer and message port. But we're going to talk about array buffer. So why would you want to use this? Well, let's say I'm uploading a file. So you might have a form. You might have a big file that you need to process. And the top there, I'm just doing a array, which is just a big unsigned integer 8 array of stuff. It's an 8meg array. It's really big, but I need to chew on it, but I don't want it to be copied. So I want to use transferable. So what I'm going to do is I'm going to do post message. But note there's a second argument to post message which is kind of goofy. It's an array of those transferables. So note you array is not transferable but the underlying array buffer is. So that's what that buffer is doing. So that you send that over and then it doesn't get copied it gets transferred. So once it gets transferred it is now owned by the worker. So if we take a look at that. So if I send it, okay, it's originally 8meg. After I send it, it's zero. So it's it's no longer valid inside the main thread. And then the main then the worker has it, it can do work on it, and then it can send it back. You see that post message, it can send it back. Same API, but it's kind of a pain in the butt because you need to keep track of these references now, which ones are valid and which ones aren't. But something to keep in mind that this is super useful for processing big files because you can just shunt it off the main thread. Cool. You might be thinking, what about shared array buffers? Those are pretty cool. Uh they were developed for Wom, but you can also pass them back and forth between web workers. They do require the atomics API for the concurrency to work. Eventually the context will be replicated across all of the workers and main thread but in order to not have concurrency issues you need to use atomics. So heart what is it not heart bleed um spectre and meltdown really ruined our day. Um we can't have nice things anymore. Um shared array buffers require a lot of security in order to be useful. So you need to have a secure context which is just HPS and then you need something called cross origin isolated which is a whole thing in order for them to be useful across browsers. So awesome if you can use them but they kind of require a little extra effort on the other side in your infrastructure to make them even work for you. So kind of a bummer but they're great but they're kind of a bummer. So you don't have to do the transferable thing. You can just post message these things back and forth. Bummer. Cool. All right. So, that's workers multi-threading. Now, we're kind of moving down. So, remember cheating, workers, multi- threading. These are your big bang for buck. Do nothing, multi-thread, and now we're going to optimize our looping. Computers like to do stuff repeatedly. So, if you can do that quickly and well, you can do good things. So, there's three strategies I want to talk about. hoisting invariance, C style loops, and cache coherency. And we'll talk about what those mean. Okay, what's a hoisting and invariant? Just means take stuff that doesn't change out of the loop. You know, it might be obvious, but reading this configuration value inside the loop is wasted access. And in JavaScript, you're not really sure if a property is a getter or just a normal property or you need to traverse up a prototype chain. So, you might think, uh, access is pretty cheap, but it might not be. So just get those out of there and bring them up out of the loop. It'll save you a lot of computation. This is Firefox and the average time with it inside of the loop was 43 milliseconds and this array is like 1 million numbers long and then with it hoisted it's 5 milliseconds. Easy win. Cool. Let's talk about C style loops. A lot of people like the dot for each in the map. These are great uh because we love functional programming, but they're also slow. So there's we can do a little better. We can do four of. We can do that same thing. We can do four of. These are also great. A little faster, but they're still slow because they're still using iterators under the hood. So you have to do that next and evaluate that over there. There's a little overhead for that. They're still faster, but they're a little slower. If you need to squeeze the absolute max performance out of a loop, you got to do C style. Cyle are the fastest. And I have an example of that here. This is Chrome and I'm doing squared sum. I'm doing this squared sum over like five million items. And average time is 58 milliseconds for that functional one. We get to 36 and then we get down to single digit really fast. So if you really need to loop over a lot of things, C style. Now I'm not saying do C style for everything. do it for the things that make sense because it's a lot harder to read and it's very easy for you to footgun. So the four of I really like the four each is nice if you know that you have a list of like 10 things doesn't really matter. Cool. Let's talk about cache coherency. Now what if I told you the one on the left is significantly faster than the one on the right? Like what do you think? Like why is that? Why do you think that you you know like well they're named fast and slow? Maybe it's that's why that's why. No, it's cache coherency. So when you read data from memory, your CPU loads that data into a cache line. So when you read a piece of data, what'll happen is it'll pull as much data, sequential data as it can into the CPU cache. And if you jump around, it needs to go back to main memory and pull new data. So the one on the right, what it's doing is it's skipping around by approximately a whole cache line each iteration. So it's just going back to memory, back to memory versus being like, "Oh, I'm in the CPU cache and I can rip through the CPU cache really fast." In fact, it's like seven times faster to do. So there's the fast one is 19 milliseconds and the slow one we're still waiting for. We're still waiting and we're waiting and we're still waiting. Wow, that was a lot. I'm doing multiple samples. I'm doing like a thousand samples. That's why it takes forever. Okay, cool. So, we talked about loops. Let's talk about data data oriented programming. This is all about just doing things in bulk. So, computers again like to do loops. They like to do things in bulk. If you knew and if you do things in bulk, you do a lot of the same thing. They don't have to guess. They can do it and have branch protection work. So you might have heard of this as like entity component systems if you ever done like video game stuff. It's kind of the it was like the hotness there for like five years. Everyone was talking about entity component systems. Now everyone's back to be like, "Oh, maybe you don't need any component system." But anyway, the things that it does really well is you avoid lastminute decision-m which can thwart your branch protector. So let's look at maybe some game code that I made up. Let's say we have a monster and it has like an ondeath handler and it has an update method that we call every tick of the frame and it's doing stuff if it's dead or if it's angry or it's doing normal stuff. It's walking around. This is not super great because now we have to loop through all of these. We have indirection. We have to go and load the monster into the cache. Then we got to go find the method in memory. We got to go call that method. The branch predictor might be wrong. You might say, "Oh, is it dead or is it angry?" These things are all problems. So, how can we make this better? Well, we can first start by working in bulk. We can say, "Okay, I'm going to go ahead and take those methods off of my monster and I'm going to put them into a bulk process and I'm going to rip through that computers. This this is a significant speed up." Like, this helps a ton. Like, now we can load all of our monster data into cache lines and then we can operate on it. We still are making decisions at the last minute, though. We got those if statements at the last minute. You know how we can get rid of those? We don't need if statements. We can just have separate arrays that implicitly encode that information. Now there's no chance the branch predictor is wrong. It's only doing one thing. It can do it really fast. And then we're still doing things in bulk. This is great. This works really well. This is how Excalibur works. This is how we we cram data through the game engine and onto the screen. like trust me this works really well. Another way you can save CPU cycles is clever data clever data structures. So let's say you have a problem like I have where you need to collide with things on the screen. Well, the naive approach is like, hey, I have a thing. It needs to collide with other things. I'll just loop through all of the things in the scene for all the other things. That's n squ. Not so great. But what you can do here is you can do like a hash. So like I have this grid. I'm going to go ahead and hash all of my colliding this grid space. There's no other colliders. I'm done. I don't have to loop through every other collider. So these are examples of clever data structures. So like and if you're doing like map software or you're doing location finding, these are things that can really help you. Code routines. So code routines are really fancy. I like them a lot. If you know me, I talk about code routines a lot. There's like a less lesser used feature in JavaScript called generators, and that's what that function star is. And I'm not going to dig too much into it, but co- routines are so awesome. They literally let you amortize computation over time. You know what that is? That's animation. Animation is computation over time. So every time you're seeing a yield here, that just means one tick. So yields just basically say hey put a pause point in my program that's perfect place for a frame an animation frame. So here's like a very simple implementation of co- routines. Um and all all you need to know is like oh these are these are frames but you don't need to do necessarily animation. You have an expensive computation you need to do. You can parcel it out with yields so that you can do a little bit of work a little bit of work so that you don't do it all at once and lock up the main thread. So co- routines are a great way for you to do a little bit of work over time and save that main thread. And there's an article on cut routines if you want and animating with co- routines on my website ericonharm.com. Cool. All right, we went through a lot of stuff. Let's talk about Wom. Wom is awesome. You should definitely use WOM if you have the use case for it. So, it has some pros. It has very consistent performance. So, one of the problems with JavaScript is this JIT thing where we're doing all of this like tom foolery to try and like get it to operate in a very consistent and fast way. But Wom just lands in this compiled ahead of time. So, we don't have to worry about our code structure as much. Wom just has very consistent performance. You can use single instruction multiple data. So this is great for like you you need to do big math problems, you know, like you have a lot of adding to do, you got a lot of multiplying to do, you got matrix multiplication to do. This is great for like maybe you're doing AI stuff, you need to do lots of vector multiplication. SIMD is the only uh WM is the only place you have SIMD security. Uh it was built with security in mind. It's got a security sandbox. In fact, a lot of people are now using it as a way of running untrusted code. And I think Cloudflare workers, they compile JavaScript into a subset of JavaScript that runs in Wom. Isn't that wild? But it's great for running untrusted code. So like if you need to run untrusted code or you're like, "Oh, I want to support every language known to man on my server, maybe compile it to Wom and then you can just run it." And that's what I think Planet Scale does. Not Planet Scale, excuse me. Time Spacetime DB. That's what I was thinking of. Spacetime DB. They let you run any code as long as it's Wom. Pretty cool. You can run any language that compiles to WOM. It's platform independent. You can reuse libraries from your favorite language. So there are a lot of really powerful C libraries for doing like video and audio work. This is a place where you can go and reuse those libraries. Uh in fact you can outperform foreign function interface calls in node. So node has this thing called ffi where you can call native libraries. But that whole machinery is slower than just running WOM often times. Not wild. So there's some cons. It's difficult to debug. Um certainly it's a it's a new language. So maybe that also comes into the into it. JS can be just as fast. So I wouldn't just use WOM because it's cool. I'd use WOM because it solves a particular problem you have, which is oh, I need to do video encoding. There's no way on earth I'm going to write my own video encoder. So that kind of thing. Uh one of the cons is uh it tends to drop out large binaries. So like even simple stuff you get like 100 KB binaries. If you do like blazer it's huge as well. So that's kind of a wmp wmp. Um marshalling data like that post message that can be expensive. So if you need to move a lot of data back and forth that can also be expensive. And here's a really good example. Uh, let's see. Squish. Let's go look at Squoosh. So, if you're looking for like a really good reference implementation for like how to use WOM, Squish by Google is a image processing software to do like random things that you want to do with your image. So, like say I want to change the color palette and I want to compress it. You know, I want some browser JPEG. I want to move back and forth so you can kind of see what maybe I want. Oh, it's not really doing anything for me. Let's do the quality. Let's make it very low. There we go. You can kind of see it. There we go. Yeah, fun stuff. But go check that out. They have a whole bunch of like C libraries, C++ libraries encoded into W in there, so you can see how they do it. All right, CPU is cool. What if we just did the GPU? You know, the GPU, I've heard that's pretty cool for like video games and stuff, you know? Why don't we use the GPU? It's awesome. So, if you have a problem that's well suited for the GPU, like particle simulations, the GPU is really good. Um, one of the problems with the GPU is you need the problem that is well suited. So, like if you're doing like four transforms, you're doing signal processing, GPUs are good. you're doing fluid dynamics, you're doing linear algebra, you're doing neural nets, computer vision, those things are really good on the GPU. Other things, not so much. So the GPU is really good at doing like massive parallel operations on math, but it's not so great at like if this, then that. If you have ifs involved, it's probably not a great task for the GPU. Uh, so you might be like, "Oh, I heard about this web GPU thing. Can I use that?" Not yet. It's like it's like fusion. It's like always five years away. I've I've been waiting for WebGu for WebGPU for so long. There's uh a thing in WebGPU called compute shaders uh which are awesome because in WebGL the way that you would do a comput shader is you'd format your problem in the language of an image. You would make a PNG that is your problem. You'd feed it into a shader program and it would spit back out an image and you would interpret that image as your result. Pretty wild. Seems kind of like a hack, but hey, that's that's comput shaders. Cool. All right. So, we've talked a lot about CPU and how CPU can get in your way and how to make that faster. Let's talk about how memory can actually slow down your app. So, uh memory in JavaScript is generational for the most part. Um, there's a few other engines that were reference counted. I think WebKit is now generational, no longer reference counted. Uh, Quick.js is reference counted. And there's a few other smaller engines that are reference counted. Reference counted is by far the easier one to implement because when the references go down to zero, you can garbage collect. Okay. What is a generational garbage collector? You're probably familiar with this if you're familiar with .NET. But the idea is you run multiple passes over all of your garbage and if it survives a pass, it gets promoted. And in the case of JavaScript, if it survives two passes, then it gets promoted to the all space versus the new space. The theory here is that most objects are shortlived. So if it's in the new space, it can get collected right away. That's a minor GC. What happens in V8 if you have a major GC is it goes and marks from the root so your like window it goes and explores all of the things from window down and if it can find a reference to it if it can find a path then it's being used if it can't then it marks it and sweeps it away we can't have memory leaks with garbage collection yes you can way worse because they're harder to spot. Uh so some symptoms here is you might have crashing, you might have some unresponsive pages, you might have jank, like the page will feel like janky. It'll like stutter. This is especially apparent in like web games. You'll see like stutters in the frame rate. Uh and they're tricky. Um there's a few ways you can leak memory. Unintentional globals, event handlers, disconnected DOM nodes, console logs. by the way, are a great way to leak memory. If you console log a big object, guess what? It's in the console now. So, maybe a reason to not use console logs as much. Cool. So, here's an example in the Firefox dev tools of a minor GC. Uh, and you can see it's saying jank. There was a minor GC and there was jank while it was doing this garbage collect. We don't want this. And this is a great way for you to be like, hey, what is happening? Why? Why? Why is my page janky? You can run the you can run the tools. Here's an example of um a memory leak as our baseline is increasing. You notice there's this classic sawtooth pattern in garbage collector languages. You know, there's a bunch of garbage gets collected. The problem is if your baseline starts going up. So this baseline on the right is starting to go up. To debug this, we can use heap snapshots. So we can go ahead to the memory tab and click heap snapshot. You get a snapshot, bunch of objects. You want to look at retained size. This is the size that is retained. This is the this is this these are the bits right there. So I'm profiling right now. I do a heap snapshot of Excalibur. I'm like, "Huh, I saw this big data array. What is this?" You know, is this a problem? And I'm taking a look at it and I'm like, "Oh, I'm pre-allocating this huge buffer to send to the graphics card. That's not a leak, but it might look like one. You know, you got a, you know, 2,000 KB buffer. You be like, maybe that's too much. Maybe I need to make some changes in Excalibur so that I'm not like pre-allocating that much. What you can also do is you can do another heap snapshot and do a comparison. And this is super useful because this lets you know if you're leaking memory over time. Say you're running your app, you take a snapshot, you run your app for a little bit more, take another snapshot, compare it. And I'm looking at this, I'm like, okay, those buffers aren't growing. I'm not getting more. I'm getting some compiled code that's coming from like the JIT. I'm seeing some heap numbers. I'm seeing some arrays, but they're mostly small. Okay, I'm not super worried. Uh, Firefox has a really cool heap snapshot. It shows you like like how big things are in memory. So like the gravitas of my array buffers is like a lot higher now. It's like, oh, you got nine megabytes of graphics card stuff. Maybe you should make that smaller. Um, there's also the performance monitor tab. You have to like go more tools to find this, but it'll give you like a live feed. This is kind of fun. If you like watching scrolling graphs like I do, this is really fun. Uh, okay. So, let's talk about how you can leak memory. So, the easiest one to do if you're not using TypeScript is the unintentional global. TypeScript will yell at you if you do this. Uh, because it'll be like, "Yo, dog, you are assigning to a global. Uh, nice try. Don't do that." Um, so like say I didn't have that var uploaded file right there. You can just unintentional global that. Like it's really easy to do this. So, you know, this happens a lot with like file. I I'm using file upload cuz I know reasons those can get big especially if you're doing things to those files in JavaScript. Yeah, this is an easy one. Uh, and maybe this is an obvious one, but I want to point it out. This is a more insidious one. This is like by far the one that happens to me in production code the most is event handler leaks. You just add event handlers to things and then you don't ever clean them up. And this also can be bugs in your code because if you add more and more event handlers, they just start stacking up. You need to remove your event handlers. Save a reference to them. Uh do what I do is I have a wrapper type that closes over the original element and uh does the remove event listener for me. Super nice. Another way to leak memory is set timeout and set interval. Uh or any closure really, but set set interval and set timeout are really easy to do. So let's say I'm uploading a file again. Uh this will leak the entire file like that won't get cleaned up because there's a closure that references it in that set timeout. You see that? That's insidious, right? like how would these are hard to spot. So if you take a look at that heap snapshot thing that I was talking about earlier, we can go and take a look at that difference that comparison and you can see ah there's that closure in that heap snapshot. I got that 100meg file. See 100meg file 1024 x 1024 by 100. Ouch. Yeah. Pretty wild, huh? Don't do that console log there. Careful what you close over in your closures. Closures are like a very insidious way to leak memory. Um, and it's probably a good practice to just like poke around every once in a while and like you boot up your app, take a snapshot, do a few things, take another snapshot, see if things look the way you think they should. Um, this one's super common, detached DOM nodes. Um the idea here is we have like a chat app and you know I'm showing elements uh by appending them to the chat element. You know this component has like a reference to a DOM node. I'm appending elements to it and then when I hide them I go ahead and push them onto an internal array but I never get rid of them. And so now I just have handles to DOM elements that are hidden. So I'm clearing them out of the DOM there with that inner HTML empty. like the DOM node is gone inside of the DOM, but I have a reference to it in JavaScript now. And it's like if I were in a code review, that might not be super immediately obvious. So, that's another one. Just be very careful with DOM nodes. They're they're easy to leak. Uh, a few notes on frameworks. Angular.js. RxJS observables are the bane of my existence. I hate them so much. Um, uh, for a lot of reasons, but also they're super easy to leak. um you need to be unsubscribing from them in your ng on destroy. Uh web components also this kind of goes back to that event handler thing. You wind up attaching a lot of event handlers in web components. You need to unattach them in the disconnected callback. Otherwise, you've just leaked event handlers. And if the event handlers clo close over something big like a final upload, you've just leaked a big file, too. Um, view you need to use the unmounted react use effect is kind of goofy. You return the cleanup closure. You see that you have use effect. You have some setup logic and you return the cleanup closure. I see a lot of use effects without that one, you know. Uh, and then ngx reactive state history. If you have that plug-in turned on and you have like infinite history in production, that can stack up and crash your production app. So maybe limit history to like 20 or something like you know you can just have your dev tool reactive state history ngx just loading up memory and crashing your app. All right so we talked about leaks. There's another memory problem in garbage collected languages called thrashing. Uh which if you were a Java person you might be very familiar with. But this is just a ton of garbage. So it's not a leak. You're just making a lot of trash. And so this kind of runs a foul of some like functional immutability patterns where you're making new arrays and new objects all the time. Um things I would recommend is you avoid object literal and array literal and loops because those are creating objects. Spread is a sneaky way of creating objects. The spread operator like if you're spread operating into another object, you've just created an object. Symptoms more jank. How do we fix how do we fix this? There's scratch and pools. Let's talk about it. So, here's an example of memory thrashing. And at the top there, you can see a lot of red triangles. And you can see a lot of garbage collection. Like you can see the characteristic saw, but the saw is real bad because look at all these minor GCs that I'm doing. I'm just creating a ton of garbage, right? And minor GCs are fired when we have lots of short-lived objects, right? Minor GCs clean up the shortlived things. So, here's that code that does that. Like, maybe I'm doing math. Like, I have a vector library where I have two components. Maybe I'm doing some AI stuff. I don't know. Uh, but I'm doing a lot of vectors. I'm creating a lot of them and I'm just throwing them away. I'm just making new vectors all the time. And that's what happens here. And actually, this is what happened in Excalibur. So, what we wound up doing uh in some spots is we use scratch variables. So, here's an example of a scratch variable. So we pre-allocate a variable. So we have the space in memory for it. We're not going to create new ones every time. And then what we wind up doing is we added this like destination property where we could just update a reference. So we could say, okay, I'm going to add two vectors together, but instead of creating a new one, I'm going to go update a reference. So that way like this code is wild to read, right? This is in that curve of like performance is hard to read. Like I have a comment in here. This code looks wild, but it's to avoid the GC. Like literally, this is in the production Excalibur code. If you go look at it, it's like this is the reason we did this wild thing. So, that's one way to do it. Scratch variables. Another way to do it is pools. So, pools are a trade-off. You pre-allocate up front uh so that you don't allocate later, but they can also become a memory leak. So, careful with pools. So, here's an example. I call it a rental pool. Uh the difference between a pool and arena in my mind is a pool is like you're taking one object out and putting it back. So that's what this is doing. So it's like I can take one object out, I pop off the stack or I can put it back in and I can push. And this is a super simple implementation that doesn't have like error checking and stuff. So beware if you use this in production. So this is like a pool. So I've pre-allocated these objects. I'm not I'm not making new ones. I'm not forcing GC. Here's an arena. The difference between a pool and arena is arenas, they all get deallocated at once. So I return every object I've taken out of the arena back at the end of a frame. So that's what we're doing here. So draw calls famously are done at the end of the frame. So this is the idea. So like I'm pulling out pre-allocated objects. I'm using up thousands and thousands of them and then at the end of the frame I'm I'm done with all of them. So that's what an arena is. So that's what like allows us to do this where we can draw like a ridiculous number of bunnies bouncing around in Marvel costumes. Okay, that's how we can avoid memory thrashing. Let's talk about the JIT. The JIT is how JavaScript is fast. So remember I was talking about this earlier. The JavaScript runtime has an interpreter. It produces bite code and then as it runs your bite code, it gets information about how your code is running over time and it can feed that back in to the optimizing compiler and produce actual legitimate machine code for your architecture. But if you do something that the JIT doesn't like, you get deoptimized and you get pushed back to interpreted land. You don't want that. Um, this is a great talk by the way um, uh, by Francisco Hinklelman. Uh, it's, uh, talks all about it. I recommend watching this talk after this talk. Super great. She goes into super deep detail about how the JIT works in Chrome. Uh, definitely check it out. All right. I don't want to play the video. >> No, don't play the video. All right. What is a legit? I've been saying that word a lot. That's just in time compiler as opposed to ahead of time compiling AOT. So like other languages like C and Rust have ahead of time. They produce the machine code. Uh JavaScript the way that it's fast is it decides to produce machine code as you're running. Now JavaScript doesn't have static types. Static types are super useful for compilers to produce ahead of time code. So the way that JavaScript does is is it watches how you use the JavaScript because it has no types. And even if it did, like if you use TypeScript, TypeScript wouldn't help because the TypeScript's type system is not sound. So it wouldn't be able to produce machine code without running it because you can always violate the TypeScript types. You want the JIT, you want it really bad. All right. So how does the JIT work? So when you're creating objects in JavaScript, they are like hidden classes or shapes or types. They're you know all of these words are used in the different engines. You can think of them as like an internal strruct. So like if I'm creating like that vector type that we saw before, first we start with the empty object type. We then we added an x then we added a y and they build this chain up uh this shape chain of objects. And this is how like prototypical inheritance works, how everything works. my clicker will work. Order matters. So if I create an object with X first then Y, it is different than if I created an object with Y first then X. TypeScript kind of helps with this because it allows it makes you build stuff the same way, you know, but if you're just doing JavaScript, it's very important that you build objects the same way. If you're doing object literal, it's very important that you build all of the properties the same way. So it's very good idea to have factory methods to combine objects so that they happen the same way. So avoid adding properties at runtime. You probably already know this, but that creates if you add a property at runtime, it creates a new hidden class. So what you really want in the JIT is monomorphic code. What does that mean? It means one shape. You want code that works on one shape and if it's one shape, it can optimize it versus megamorphic code which is many shapes. So this function of lucky charms, hearts and rainbows and blue moons. All right. So here's an example uh from uh that talk I mentioned earlier is we do a comparison to the type and if the type is not does not match that shape does not match we bail to line 5A which goes back to the interpreted code. So otherwise we can just do that RX + 17 offset. we can just jump into memory where the property is. So here's an example of that. So I've created a vector X and Y. I have a more different vector which is Y than X. And then we can do a lot of iteration over it really fast. So I'm just creating vectors. I'm using random to thwart the compiler from trying to optimize things. Um so I'm going to sum all the squares and I'm going to do it again and see what happens. Did you know that you can tell V8 to dump out when it deoptimizes? You can do node trace opt and you can see when V8 decides to optimize or deoptimize. And you can see right when I hit that second for loop, it needs to deoptimize the function because it saw a new shape. You see that some square is deoptimized and then it gets reoptimized later after it it warms up again. But I got deopted for that whole for loop until it got hot again. And then V8 was like, "Oh, we should probably optimize this again." Now, it's not like an all or nothing thing. V8 can handle megamorphic code. It can handle polymorphic code. Um, it just takes more time. It'll wind up optimizing a function that can handle both XY and YX, but We hit a deop case which is just going to hitch in our app. >> Cool, right? Did you know you can do that? Cool. All right. So, that's legit. Uh, if you're down there, that's pretty wild. You know, I think that's really squeezing blood from the rock, but you might need to be there. Uh, depending on what you're doing, you might need to be there if you're doing realtime applications. So, how do we prevent regressions? We've done all this work. I'll let you know once we've done performance optimization it's pretty unstable like it's pretty rickety you know like there's really like very subtle changes cause very big differences in performance characteristics and it's not immediately clear so ways to do this you got to run benchmarking tests in your CI if it's important to you like your core modules need to be either running playright tests or your own custom benchmarking scripts on your own hardware so you can go apples to apples if you do like the cloudr runner it's too variable to compare. You're just so wildly different between runs. So like I have a hosted runner in my basement that I used to run benchmarks. Like that's what I do. And it's a bad machine, but it runs the same every time. It doesn't need to be an awesome machine. It just needs to be the same machine so that you can compare run between run. And that's what I do. All right. So I have a few final thoughts here. Um, remember, set a goal and apply science. Don't just keep digging because you can. Like, I have definitely not followed my own advice and I've just dug and dug and dug and and like it's performant, but at what cost? Uh, measure twice, cut once. Focus on the big fish. Don't optimize things that aren't really taking up much time. And uh after this, if you would like to rate the talk, I have a preferred color. It's that one. Um I have a preferred color, but if you were going to give me a different color, uh please send me some feedback. Uh that's my contact form. I'll get an email. Um so if there's something you wanted to see different or you disagreed or you want different information, you let me know. Uh there's a ton of resources here. This is a QR code for the slides. I'm a big QR code fan. So if you want the slides later, you can see all my my wild notes um and nonsense, but I really recommend the front-end masters courses on this. So it's paid, but the uh the courses from John Cooperman on dev tools are really good. So he'll dive really really deep in how to use the dev tools. uh Steve Kenny and Mishko go really really really deep into like the JIT and how like JavaScript runs and then uh Prime's course actually focuses a lot on that CPU and memory side of things. So I really recommend taking a look at that. There's a couple really good blog posts and a really good video there life of a script from V8 engineers. So definitely recommend that. Super awesome. And I think that's all I have. Yeah, that's it. Thank you.